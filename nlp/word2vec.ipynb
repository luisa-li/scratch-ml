{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A tiny word2vec model for learning.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"A tiny word2vec model for learning.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import math \n",
    "import random \n",
    "from nlp import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/ptb/ptb.train.txt\") as f:\n",
    "    raw_text = f.read()\n",
    "sentences = [line.split() for line in raw_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6719"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab(sentences, min_freq=10)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_probability(t: float, freq: int, num_tokens: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the probability for this word to be discarded. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : float\n",
    "        Hyperparameter to adjust for subsampling.\n",
    "    freq : int\n",
    "        Frequency of the word in the corpus.\n",
    "    num_tokens : int\n",
    "        Total number of tokens in the corpus.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The probability for discarding this word.\n",
    "    \"\"\"\n",
    "    return max(1 - math.sqrt(t / (freq / num_tokens)), 0)\n",
    "\n",
    "def keep(prob: float) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if this word is kept under the roll of a imaginary dice.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prob : float\n",
    "        Probability for keeping\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        To keep or not to keep :)\n",
    "    \"\"\"\n",
    "    rand = random.uniform(0, 1)\n",
    "    return rand < prob\n",
    "\n",
    "def subsample(sentences: list[list[str]], unk: str) -> tuple[list[list[str]], collections.Counter]:\n",
    "    \"\"\"\n",
    "    Subsamples the words in the vocabulary according to their frequencies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words : list[list[str]]\n",
    "        All the words in the corpus\n",
    "    unk : str\n",
    "        The <unk> token in this case\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[list[list[str]], collections.Counter]\n",
    "        The subsampled words and the counter\n",
    "    \"\"\"\n",
    "    sentences = [[token for token in line if token != unk] for line in sentences]\n",
    "    counter = collections.Counter([token for line in sentences for token in line])\n",
    "    num_tokens = sum(counter.values())\n",
    "    subsampled_sentences = [[token for token in line if keep(discard_probability(T, counter[token], num_tokens))] for line in sentences]\n",
    "    return subsampled_sentences, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled, counter = subsample(sentences, '<unk>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting center words and context words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    \"\"\"Return center words and context words in skip-gram.\"\"\"\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        # adding the context for each word in that line\n",
    "        # such that each element in center corresponds to a sublist of contexts\n",
    "        for i in range(len(line)): \n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i - window_size),\n",
    "                                 min(len(line), i + 1 + window_size)))\n",
    "            indices.remove(i)\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the total number of center context pairs would thus be the length of the flattened context list, each each word in that flattened list can be paired with a center word\n",
    "all_centers, all_contexts = get_centers_and_contexts(subsampled, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax objective is too expensive to compute, negative sampling modifies the learning objective to make the learning problem much easier to approach. \n",
    "\n",
    "The problem is now, given two words, predict if they are context target pairs. We just extracted positive context target pair, and now we want negative examples - words that are not context target pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a logistic model, where c is the context word, t is the target word, and y is the label.\n",
    "\n",
    "$ P(y = 1 | c, t) = \\sigma (\\theta_t^T e_c) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3 # number of negative examples per positive example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much more simplified version of the algorithm that draws negative samples from the one described in the paper. Instead of drawing negative samples from a specific distribution, we are just going to randomly draw K of them from outside the context window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(all_centers, all_contexts, sentences, K):\n",
    "    all_negatives = []\n",
    "    for center, context, sentence in zip(all_centers, all_contexts, sentences):\n",
    "        difference = [word for word in sentence if word not in context and word != center]\n",
    "        all_negatives.append(difference[:K])\n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negatives = get_negative_samples(all_centers, all_contexts, subsampled, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at some of the negatives and contexts just to make sure they look right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['as', 'a', 'director'],\n",
       " ['mr.', 'is', 'chairman'],\n",
       " ['N', 'years', 'old'],\n",
       " ['of', 'used', 'to']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_negatives[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['years', 'will', 'the', 'board'],\n",
       " ['N', 'will', 'the', 'board'],\n",
       " ['N', 'years', 'the', 'board', 'as', 'a', 'director'],\n",
       " ['will', 'board'],\n",
       " ['N', 'years', 'will', 'the', 'as', 'a', 'director', 'N']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_contexts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the dataset with negatives and contexts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
